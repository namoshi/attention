{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 10, 1, 1], expected input[128, 128, 1, 1] to have 10 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 150\u001b[0m\n\u001b[1;32m    146\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    148\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 150\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m net(inputs)  \u001b[38;5;66;03m# アテンションマップはここでは使用しない\u001b[39;00m\n\u001b[1;32m    151\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    152\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 100\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m flattened_conv3 \u001b[38;5;241m=\u001b[39m conv3_out\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# B x 128 x (16*16)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m avg_pool_conv3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(flattened_conv3, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# B x 128 x 1 x 1\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m td_conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_conv(avg_pool_conv3)) \u001b[38;5;66;03m# B x 128 x 1 x 1\u001b[39;00m\n\u001b[1;32m    101\u001b[0m td_upsampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_upsample(td_conv_out) \u001b[38;5;66;03m# B x 128 x 2 x 2\u001b[39;00m\n\u001b[1;32m    102\u001b[0m h, w \u001b[38;5;241m=\u001b[39m conv3_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], conv3_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 10, 1, 1], expected input[128, 128, 1, 1] to have 10 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "# CIFAR-10データセットの準備\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Self-Attention層 (変更なし)\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  # Attention weight\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, height, width = x.size()\n",
    "        proj_query = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1)  # B x (H*W) x C'\n",
    "        proj_key = self.key(x).view(batch_size, -1, height * width)  # B x C' x (H*W)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # batch matrix multiplication: B x (H*W) x (H*W)\n",
    "        attention = self.softmax(energy)  # B x (H*W) x (H*W)\n",
    "        proj_value = self.value(x).view(batch_size, -1, height * width)  # B x C x (H*W)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1)).view(batch_size, C, height, width)  # B x C x H x W\n",
    "\n",
    "        return self.gamma * out + x, attention\n",
    "\n",
    "# CNNモデルの定義 (pool1とpool3を除去)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "        # Top-down attention用のパラメータ\n",
    "        self.td_conv = nn.Conv2d(10, 128, kernel_size=1) # 出力クラス数から特徴マップへ\n",
    "        self.td_relu = nn.ReLU()\n",
    "        self.td_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.td_conv_final = nn.Conv2d(128, 1, kernel_size=1) # 最終的なアテンションマップ\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1_out = self.relu1(self.conv1(x))\n",
    "        conv2_out = self.relu2(self.conv2(conv1_out))\n",
    "        pool2_out = self.pool2(conv2_out)\n",
    "        conv3_out = self.relu3(self.conv3(pool2_out))\n",
    "\n",
    "        # Top-down attention calculation\n",
    "        batch_size = x.size(0)\n",
    "        # 各クラスのロジットを特徴マップの空間に変換 (fc2の出力はまだ計算されていないので、ここでは仮のテンソル)\n",
    "        # 実際のロジットは fc2 の出力\n",
    "        # td_input = torch.randn(batch_size, 10, 1, 1).to(x.device) # 仮の入力\n",
    "        flattened_conv3 = conv3_out.view(batch_size, 128, -1) # B x 128 x (16*16)\n",
    "        avg_pool_conv3 = torch.mean(flattened_conv3, dim=2).unsqueeze(-1).unsqueeze(-1) # B x 128 x 1 x 1\n",
    "        td_conv_out = self.td_relu(self.td_conv(avg_pool_conv3)) # B x 128 x 1 x 1\n",
    "        td_upsampled = self.td_upsample(td_conv_out) # B x 128 x 2 x 2\n",
    "        h, w = conv3_out.shape[2], conv3_out.shape[3]\n",
    "        td_upsampled_resized = nn.functional.interpolate(td_upsampled, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        attention_map = torch.sigmoid(self.td_conv_final(td_upsampled_resized)) # B x 1 x H x W\n",
    "        attention_map = attention_map.expand_as(conv3_out) # B x 128 x H x W\n",
    "\n",
    "        print(conv3_out.shape)\n",
    "        print(attention_map.shape)\n",
    "        \n",
    "        # relu3の結果とアテンションマップを乗算\n",
    "        attended_conv3 = conv3_out * attention_map\n",
    "\n",
    "        flattened = attended_conv3.view(-1, 128 * 16 * 16)\n",
    "        fc1_out = self.relu4(self.fc1(flattened))\n",
    "        logits = self.fc2(fc1_out)\n",
    "\n",
    "        return logits, attention_map.mean(dim=1) # チャンネル次元を平均して返す\n",
    "\n",
    "    def get_features(self, x):\n",
    "        conv1_out = self.relu1(self.conv1(x))\n",
    "        conv2_out = self.relu2(self.conv2(conv1_out))\n",
    "        pool2_out = self.pool2(conv2_out)\n",
    "        conv3_out = self.relu3(self.conv3(pool2_out))\n",
    "        return conv3_out\n",
    "\n",
    "# モデルのインスタンス化\n",
    "net = Net().to(device)\n",
    "\n",
    "# 損失関数と最適化関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# 学習の記録用リスト\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, _ = net(inputs)  # アテンションマップはここでは使用しない\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    # テストフェーズ\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs, _ = net(images)  # アテンションマップはここでは使用しない\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_loss / len(testloader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# 学習曲線のプロット (変更なし)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Testing Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 中間層の特徴ベクトルの可視化\n",
    "net.eval()\n",
    "all_features = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "        if i >= 10:  # 最初の10バッチのみ使用 (計算量を抑えるため)\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        features = net.get_features(images).cpu().numpy()\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_features = np.concatenate(all_features, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(all_features.reshape(all_features.shape[0], -1)) # 3D特徴マップを2Dに変換\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = plt.cm.get_cmap('tab10', len(classes)) # または plt.cm.get_cmap('Set1', len(classes)) など\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    indices = np.where(all_labels == i)[0]\n",
    "    plt.scatter(principal_components[indices, 0], principal_components[indices, 1],\n",
    "                label=classes[i], alpha=0.6, c=[colors(i)])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Intermediate Layer Features')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 学習後のアテンションの可視化 (変更)\n",
    "def visualize_attention_on_image(model, image, original_image, class_names):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 入力画像をデバイスに移動し、バッチ次元を追加\n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "        output, attention_map = model(image_tensor)\n",
    "        _, predicted_idx = torch.max(output, 1)\n",
    "        predicted_class = class_names[predicted_idx.item()]\n",
    "\n",
    "        # アテンションマップをCPUとNumPy配列に移動\n",
    "        attention_map = attention_map.squeeze(0).cpu().numpy()\n",
    "        # アテンションマップのサイズは (H x W)\n",
    "        attn_map = attention_map # 既に (H x W) の形状になっているはず\n",
    "\n",
    "        # ヒートマップのサイズを入力画像のサイズに合わせる\n",
    "        original_height, original_width = original_image.shape[1], original_image.shape[2]\n",
    "        attn_map_resized = cv2.resize(attn_map, (original_width, original_height))\n",
    "\n",
    "        # 0から1の範囲に正規化\n",
    "        attn_map_normalized = (attn_map_resized - np.min(attn_map_resized)) / (np.max(attn_map_resized) - np.min(attn_map_resized) + 1e-8) # ゼロ除算回避\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * attn_map_normalized), cv2.COLORMAP_JET)\n",
    "        heatmap_rgb = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # オリジナル画像をRGBに変換\n",
    "        original_image_rgb = (original_image.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5) * 255\n",
    "        original_image_rgb = original_image_rgb.astype(np.uint8)\n",
    "\n",
    "        # サイズが一致しているか確認 (デバッグ用)\n",
    "        if original_image_rgb.shape[:2] != heatmap_rgb.shape[:2] or original_image_rgb.shape[2] != 3 or heatmap_rgb.shape[2] != 3:\n",
    "            print(f\"Error: Shape mismatch before overlaying.\")\n",
    "            print(f\"Original Image Shape: {original_image_rgb.shape}\")\n",
    "            print(f\"Heatmap Shape: {heatmap_rgb.shape}\")\n",
    "            return\n",
    "\n",
    "        # ヒートマップをオーバーレイ\n",
    "        alpha = 0.5  # 透明度\n",
    "        overlay = cv2.addWeighted(original_image_rgb, 1 - alpha, heatmap_rgb, alpha, 0)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(overlay)\n",
    "        plt.title(f'Top-Down Attention Overlay (Predicted: {predicted_class})')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# テストデータからランダムなサンプルを選択\n",
    "num_samples_to_visualize = 5\n",
    "indices = random.sample(range(len(testset)), num_samples_to_visualize)\n",
    "for index in indices:\n",
    "    img, label = testset[index]\n",
    "    original_img = img.clone() # 可視化用にオリジナルのテンソルを保持\n",
    "    class_name = classes[label]\n",
    "    print(f\"Visualizing top-down attention for sample with label: {class_name}\")\n",
    "    visualize_attention_on_image(net, img, original_img, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
